---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---


 You can also find my articles on <u><a href="https://scholar.google.com/citations?user=-GPPICQAAAAJ&hl=en">Google Scholar</a>.</u>

<h2>Publications</h2>
<ol reversed>
	<h3>Under Review</h3>
			<li> Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models </li> [<a href="https://arxiv.org/abs/2510.04618"> Paper Link </a>] <br />
			<li> SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators </li> [<a href="https://arxiv.org/abs/2511.03092"> Paper Link </a>] <br />
			<li> Distributed LLMs and Multimodal Large Language Models: A Survey on Advances, Challenges, and Future Directions </li>
	<h3>Technical Report</h3>
			<li> EvaByte: Efficient Byte-level Language Models at Scale </li> [<a href="https://hkunlp.github.io/blog/2025/evabyte/"> Blog Link </a>]<br />
			<li> SubgoalXL: Subgoal-based Expert Learning for Theorem Proving </li> [<a href="https://arxiv.org/abs/2408.11172">Paper Link</a>]<br />
			<li> Composition of Experts: A Modular Compound AI System Leveraging Large Language Models </li>[<a href="https://arxiv.org/abs/2412.01868"> Paper Link </a>] <br />
	<h3>Published Papers and Book Chapters</h3>
			<li> Synthetic Document Question Answering in Hungarian <br /> [<a href=""> Paper Link </a>][<a href="https://sites.google.com/view/vlms4all">Workshop Link </a>]<br /> <b>CVPR 2025 Workshop on Vision Language Models For All: Building Geo-Diverse and Culturally Aware Vision-Language Models</b>
(VLMs-4-All) </b></li>
			<li> Training Domain Draft Models for Speculative Decoding: Best Practices and Insights <br /> [<a href="https://arxiv.org/abs/2503.07807"> Paper Link </a>][<a href="https://scope-workshop.github.io/">Workshop Link</a>] <br /><b> ICLR 2025 Workshop on
Scalable Optimization for Efficient and Adaptive Foundation Models
(SCOPE) </b></li>
			<li> LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for Efficient Long-Context Inference <br /> [<a href="https://arxiv.org/abs/2503.08879"> Paper Link </a>][<a href="https://sites.google.com/view/sllm-iclr-2025/call-for-papers">Workshop Link</a>] <br /><b> ICLR 2025 Workshop on Sparsity in LLMs (SLLM)
(SLLM) </b></li>
			<li> SambaLingo: Teaching Large Language Models New Languages <br /> [<a href="https://arxiv.org/abs/2404.05829">Paper Link</a>][<a href="https://sigtyp.github.io/ws2024-mrl.html">Workshop Link</a>] <br /><b>4th Multilingual Representation Learning (MRL) Workshop at 2024 Conference on Empirical Methods in Natural Language Processing</b> </li>
			<li> Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge <br /> [<a href="https://arxiv.org/abs/2408.08808"> Paper Link </a>][<a href="https://customnlp4u-24.github.io/"> Workshop Link</a>]<br /><b>Won the best paper award</b></B><br /> <b> Workshop on Customizable NLP (CustomNLP4U) at 2024 Conference on Empirical Methods in Natural Language Processing</b></li>
			<li> Bloom: A 176b-parameter open-access multilingual language model <br /> (<b> Journal of Machine Learning Research (JMLR)</b>, 2024 (to appear) ) </li>
			<li> SambaNova SN40L: Scaling the AI Memory Wall with Dataflow and Composition of Experts (<b> MICRO 2024</b>, (To appear)) <br> 57th IEEE/ACM International Symposium on Microarchitecture <br> [<a href="https://microarch.org/micro57/"> Conference </a>][Paper] </li>
			<li> Composition of Experts on the SN40L Reconfigurable Dataflow Unit (<b>IEEE Micro Magazine</b>, (To appear)) <br> [<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=40"> Journal </a>][Paper] </li></li>
			<li> Efficiently adapting pretrained language models to new languages (<b>2023 ENLSP-NeurIPS III Workshop</b>) <br> Zoltan Csaki, Pian Pawakapan, <b> Urmish Thakker</b>, Qiantong Xu <br> The Third NeurIPS Workshop on Efficient Natural Language and Speech Processing 202 <br> [<a href="https://neurips2023-enlsp.github.io/accepted_papers.html"> Workshop </a>][<a href="https://arxiv.org/abs/2311.05741"> Paper</a>] </li>
			<li> Training Large Language Models efficiently with Sparsity and Dataflow </i> (<b> SNN Workshop at ICLR 2023 </b>) <br> Venkat Srinivasan, Darshan Gandhi, <b>Urmish Thakker</b>, Raghu Prabhakar <br> ICLR 2023 Workshop on Sparsity in Neural Networks <br> Links [<a href="https://www.sparseneural.net/home"> Workshop </a>][<a href="https://arxiv.org/abs/2304.05511"> Paper</a>][<a href="https://drive.google.com/file/d/1lKvgs9M_m43ymI-AodpotfGmJvfHsP5W/view"> Poster</a>]
			<li> Federated Learning for Resource-Constrained IoT Devices: Panoramas and State-of-the-art, Book Name: Federated and Transfer Learning, Publisher: Springer, Oct 2022 [<a href="https://link.springer.com/chapter/10.1007/978-3-031-11748-0_2"> Link </a>] </li>
			<li> PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts <br> ACL Demos Track 2022 <br> [<a href="https://arxiv.org/abs/2202.01279">Arxiv Link</a>]</li>
			<li> Multitask Prompt Tuning Enables Zero-Shot Task Generalization (<b> ICLR 2022 </b>) <br> Paper - https://arxiv.org/abs/2110.08207 <br> Model - https://huggingface.co/bigscience/T0pp <br> Github link to dataset - https://github.com/bigscience-workshop/promptsource/
			<br> The Tenth International Conference on Learning Representations, April 2022
			<br />
			<li> <i> MLPerf Tiny Benchmark </i> <br> Colby Banbury, Vijay Janapa Reddi, Peter Torelli, Jeremy Holleman, Nat Jeffries, Csaba Kiraly, Pietro Montino, David Kanter, Sebastian Ahmed, Danilo Pau, <b> Urmish Thakker </b>, Antonio Torrini, Peter Warden, Jay Cordaro, Giuseppe Di Guglielmo, Javier Duarte, Stephen Gibellini, Videet Parekh, Honson Tran, Nhan Tran, Niu Wenxu, Xu Xuesong (<b> NeurIPS 2021 </b>) <br>[<a href="https://arxiv.org/abs/2106.07597">arxiv link</a>][<a href="https://openreview.net/forum?id=8RxxwAut1BI">Openreview Link</a>]
			<br> Thirty-fifth Conference on Neural Information Processing Systems, Dec 2021
			<br />
			<li><i> MicroNets: Neural Network Architectures for Deploying TinyML Applications on Commodity Microcontrollers </i> (<b> MLSys 2021 </b>)<br> Colby Banbury, Chuteng Zhou, Igor Fedorov, Ramon Matas Navarro, <b>Urmish Thakkar</b>, Dibakar Gope, Vijay Janapa Reddi, Matthew Mattina, Paul N. Whatmough <br> 
			[<a href="https://arxiv.org/abs/2010.11267">Arxiv</a>][<a href="https://proceedings.mlsys.org/paper/2021/hash/a3c65c2974270fd093ee8a9bf8ae7d0b-Abstract.html">MLSys Link</a>] <br> Fourth Conference on Machine Learning and Systems, April 2021
			<br />
			<li><i> Doping: A Technique for Extreme Compression of LSTM Models using Sparse Additive Matrices</i>(<b> MLSys 2021 </b>)<br> <b> Urmish Thakker </b>, Paul N. Whatmough, Zhi-Gang Liu, Matthew Mattina, Jesse Beu <br> [<a href="https://proceedings.mlsys.org/paper/2021/hash/a3f390d88e4c41f2747bfa2f1b5f87db-Abstract.html"> MLSys Link </a>][<a href="https://arxiv.org/abs/2102.07071"> Arxiv Link</a>] <br> Fourth Conference on Machine Learning and Systems, April 2021
			<br />
			<li> <i> Compressing RNNs to Kilobyte Budget for IoT Devices Using Kronecker Products </i> (<b> JETC 2021 </b>) <br> <b>Urmish Thakker</b>, Jesse Beu, Dibakar Gope, Chu Zhou, Igor Fedorov, Ganesh Dasika and Matthew Mattina <br> To appear, ACM Journal on Emerging Technologies in Computing Systems, 2021
			<br> [<a href="https://arxiv.org/abs/1906.02876">Arxiv Link</a>][<a href="https://dl.acm.org/doi/abs/10.1145/3440016">ACM Link</a>] 
			<br />
			<li> <i> Federated Learning for Resource-Constrained IoT Devices: Panoramas and State-of-the-art </i> <br> Ahmed Imteaj, <b>Urmish Thakker</b>, Shiqiang Wang, Jian Li and M. Hadi Amini <br> (<b> IOTJ 2021 </b>)
			[<a href="https://arxiv.org/abs/2002.10610">Arxiv</a>] [<a href="https://ieeexplore.ieee.org/document/9475501"> IEEE Link </a>]<br> IEEE Internet of Things Journal, July 2021
			<br />
			<li> <i> Rank and Run-time aware compression of NLP Applications </i> (<b> SustaiNLP-EMNLP 2020 </b>) <br> <b>Urmish Thakker</b>, Jesse Beu, Dibakar Gope, Ganesh Dasika and Matthew Mattina <br> First Workshop on Simple and Efficient Natural Language Processing at The Conference on Empirical Methods in Natural Language Processing (EMNLP), Nov 2020 <br> Links [<a href="https://sites.google.com/view/sustainlp2020/home">Workshop</a>][<a href="https://arxiv.org/abs/2010.03193"> Arxiv Paper </a>][<a href="https://aclanthology.org/2020.sustainlp-1.2/">ACL Link</a>] 
<br />
			<li> <i> Pushing the Envelope of Dynamic Spatial Gating technologies </i> (<b> AIChallengeIoT 2020 </b>) <br> Xueqin Huang, <b> Urmish Thakker </b>, Dibakar Gope, Jesse Beu <br> 2nd International Workshop on Challenges in Artificial Intelligence and Machine Learning for Internet of Things at ACM SenSys, Nov 2020 <br> Links [<a href="https://aichallengeiot.github.io/">Workshop</a>][<a href="https://dl.acm.org/doi/10.1145/3417313.3429380">ACM Link </a>]
			<br />
	<li> <i> Understanding the Impact of Dynamic Channel Pruning on Conditionally Parameterized Convolutions </i> (<b> AIChallengeIoT 2020 </b>) <br> Ravi Raju, Dibakar Gope, <b> Urmish Thakker </b>, Jesse Beu <br> 2nd International Workshop on Challenges in Artificial Intelligence and Machine Learning for Internet of Things at ACM SenSys, Nov 2020 <br> Links [<a href="https://aichallengeiot.github.io/">Workshop</a>][<a href="https://dl.acm.org/doi/10.1145/3417313.3429381"> ACM Link </a>]
			<br />
			<li> <i> Ternary MobileNets via Per-Layer Hybrid Filter Banks </i> (<b> Joint Workshop on Efficient Deep Learning in Computer Vision </b> ) <br> Dibakar Gope, Jesse Beu, <b> Urmish Thakker </b>, Matthew Mattina <br> Joint Workshop on Efficient Deep Learning in Computer Vision at Conference on Computer Vision and Pattern Recognition (CVPR), June 2020 <br> Links [<a href="https://workshop-edlcv.github.io/">Workshop</a>][<a href="https://arxiv.org/abs/1911.01028">Paper</a>]
			<br />
			<li> <i>Pushing the limits of RNN Compression</i> (<b>NeurIPS-EMC2 2019</b>)<br><b>Urmish Thakker</b>, Igor Fedorov, Jesse Beu, Dibakar Gope, Chu Zhou, Ganesh Dasika and Matthew Mattina <br> 
5th Workshop on Energy Efficient Machine Learning and Cognitive Computing, Co-located with the 33rd Conference on Neural Information Processing Systems (NeurIPS), Dec. 2019. <br>
			Links [<a href="https://www.emc2-ai.org/neurips-19">Workshop</a>][<a href="https://arxiv.org/abs/1910.02558">Arxiv Paper</a>][<a href="https://ieeexplore.ieee.org/document/9463545"> IEEE Link</a>]
			<br />
			<li> <i>Skipping RNN State Updates without Retraining the Original Model*</i> (<b>SenSys-ML 2019</b>)<br>
			Jin Tao, <b>Urmish Thakker</b>, Ganesh Dasika, Jesse Beu <br> 
			1st Workshop on Machine Learning on Edge in Sensor Systems (Sensys-ML), Co-located with 17th ACM Conference on Embedded Networked Sensor Systems (SenSys 2019), Nov. 2019<br>
			Links [<a href="https://sensysml.github.io/index">Workshop</a>][<a href="https://dl.acm.org/citation.cfm?id=3362965">Paper</a>]<br>
			*Won the best paper award
			<br />
			<li> <i>Run-Time Efficient RNN Compression for Inference on Edge Device</i> (<b>ISCA-EMC2 2019</b>)<br>
			<b>Urmish Thakker</b>, Jesse Beu, Dibakar Gope, Ganesh Dasika and Matthew Mattina <br>
			4th Workshop on Energy Efficient Machine Learning and Cognitive Computing for Embedded Applications (EMC2), Co-located with the 46th Int. Symp on Computer Architecture (ISCA), Jun. 2019. <br>
			Links [<a href="https://www.emc2-ai.org/isca-19">Workshop</a>][<a href="https://arxiv.org/abs/1906.04886">Paper</a>]
			<br />
			<br />
	<h3>Peer Reviewed Workshop Papers</h3>
			<li> <i> Doping: A Technique for Extreme Compression of LSTM Models using Sparse Additive Matrices </i> (<b> SNN Workshop 2021 </b>) <br> <b>Urmish Thakker</b>, Paul Whatmough, Zhi-Gang Liu, Matthew Mattina, Jesse Beu <br> Sparsity in Neural Networks: Advancing Understanding and Practice, July 2021 <br> Links [<a href="https://sites.google.com/view/sparsity-workshop-2021/">Workshop</a>] <br>
			<li> <i> Doped Structured Matrices for Extreme Compression of LSTM Models </i> (<b> SustaiNLP-EMNLP 2020 </b>) <br> <b>Urmish Thakker</b>, Paul Whatmough, Zhi-Gang Liu, Matthew Mattina, Jesse Beu <br> First Workshop on Simple and Efficient Natural Language Processing at The Conference on Empirical Methods in Natural Language Processing (EMNLP), Nov 2020 <br> Links [<a href="https://sites.google.com/view/sustainlp2020/home">Workshop</a>] <br>
			<li> <i> Benchmarking TinyML Systems: Challenges and Direction* </i> (<b>Benchmarking Machine Learning Workloads on Emerging Hardware Workshop</b>)<br> Colby Banbury , Vijay Janapa Reddi , Will Fu , Max Lam , Amin Fazel , Jeremy Holleman , Xinyuan Huang , Robert Hurtado , David Kanter , Anton Lokhmotov , David Patterson , Danilo Pau , Jeff Sieracki , Jae-Sun Seo , <b> Urmish Thakkar</b>, Marian Verhelst , Poonam Yadav  <br> First International Workshop on Benchmarking Machine Learning Workloads on Emerging Hardware at Third Conference on Machine Learning and Systems (MLSys), March 2020 <br>*As part of the TinyML Performance Working Group<br>Links [<a href="https://memani1.github.io/challenge20/">Workshop</a>][<a href="https://arxiv.org/abs/2003.04821v1">Paper</a>]
			<br />
			<li> <i> Compressing Language Models using Doped Kronecker Products</i> (<b>On-device Intelligence Workshop</b>)<br> <b> Urmish Thakker </b>, Paul Whatmough, Matthew Mattina, Jesse Beu <br> On-device Intelligence Workshop at Third Conference on Machine Learning and Systems (MLSys), March 2020 <br> Links [<a href="https://research.fb.com/programs/on-device-intelligence-workshop/#Accepted_Submissions">Workshop</a>][<a href="https://arxiv.org/abs/2001.08896">Paper</a>][<a href="https://www.youtube.com/watch?v=ZjrQMaMCNLc">Video</a>]
			<br />
			<li> <i>A Static Analysis-based Cross-Architecture Performance Prediction Using Machine Learning </i> (<b>ISCA-AIDArc 2019</b>)<br>
			Newsha Ardalani, <b>Urmish Thakker</b>, Aws Albarghouthi, Karu Sankaralingam <br>
			2nd International Workshop on AI-assisted Design for Architecture co-located with 46th Int. Symposium on Computer Architecture (ISCA), Jun. 2019<br>
			Links [<a href="https://eecs.oregonstate.edu/aidarc/">Workshop</a>][<a href="https://arxiv.org/abs/1906.07840">Paper</a>]
			<br />			
			<li> <i>Measuring scheduling efficiency of RNNs for NLP applications</i> (<b>ISPASS-Fasthpath 2019</b>)<br>
			<b>Urmish Thakker</b>, Ganesh Dasika, Jesse Beu, Matthew Mattina <br>
			6th edition of International Workshop on Performance Analysis of Machine Learning Systems (Fastpath) co-located with IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), March 2019. <br>
			Links [<a href="https://researcher.watson.ibm.com/researcher/view_group.php?id=9888">Workshop</a>][<a href="https://arxiv.org/abs/1904.03302">Paper</a>]
			<br />
			<br />
	
	<h3>Extended Abstracts/ Posters</h3>
			<li> <i>Composition of Experts: A Compound AI Systems Approach to build Large Language Models</i> <br> Compound AI Systems Workshop 2024 at Data + AI Summit [<a href="https://sites.google.com/view/compound-ai-systems-workshop/home">Link to Workshop</a>]
			<li> <i> Hardware Aware Dynamic Inference Technologies </i> (<b>tinyML 2021</b>)<br>
				tinyML Summit 2021 
				[<a href="https://www.tinyml.org/event/summit-2021/">Link to Summit</a>][<a href="https://www.youtube.com/watch?v=2NyCIypmuzI&ab_channel=tinyML">Recorded Talk</a>]
			<br />
			<li> <i> Improving accuracy of neural networks compressed using fixed structures via doping </i> (<b>tinyML 2020</b>)<br> <b> Urmish Thakker </b>, Ganesh Dasika, Paul Whatmough, Matthew Mattina, Jesse Beu <br> 
			tinyML Summit 2020
			[<a href="https://www.tinymlsummit.org/">Link to Summit</a>][<a href="https://github.com/Urmish/urmish.github.io/blob/master/_publications/DKP_TinyML_Poster.pdf">Poster</a>]
			<br />
			<li> <i> Aggressive Compression of MobileNets Using Hybrid Ternary Layers </i> (<b>tinyML 2020</b>) <br> Dibakar Gope, Jesse Beu, <b> Urmish Thakker </b>, and Matthew Mattina <br> 
			tinyML Summit 2020
				[<a href="https://www.tinymlsummit.org/">Link to Summit</a>][<a href="https://github.com/Dibakar/dibakar.github.io/blob/master/Posters/tinyML%20Summit%202020%20Poster.pdf">Poster</a>]
			<br />
			<li> <i>RNN Compression using Hybrid Matrix Decomposition</i> (<b>tinyML 2019</b>)<br> 
			<b>Urmish Thakker</b>, Ganesh Dasika, Jesse Beu, Dibakar Gope, and Matthew Mattina <br>
			tinyML Summit, Mar. 2019. <br>
			Links [<a href="https://tinymlsummit.org/2019/">Link to Summit</a>][<a href="https://github.com/Urmish/urmish.github.io/blob/master/_publications/HMD_TinyML.pdf">Poster</a>]
			<br /> <br />

</ol>
