---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---


 You can also find my articles on <u><a href="https://scholar.google.com/citations?user=-GPPICQAAAAJ&hl=en">Google Scholar</a>.</u>

<h2>Publications</h2>
<ol reversed>
	<h3>Under Review</h3>
	<h3> Book Chapters </h3>
			<li> Federated Learning for Resource-Constrained IoT Devices: Panoramas and State-of-the-art, Book Name: Federated and Transfer Learning, Publisher: Springer, To Appear, 2022 </li>
	<h3>Published Papers</h3>
			<li> PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts <br> ACL Demos Track 2022 <br> [<a href="https://arxiv.org/abs/2202.01279">Arxiv Link</a>]</li>
			<li> Multitask Prompt Tuning Enables Zero-Shot Task Generalization (<b> ICLR 2022 </b>) <br> Paper - https://arxiv.org/abs/2110.08207 <br> Model - https://huggingface.co/bigscience/T0pp <br> Github link to dataset - https://github.com/bigscience-workshop/promptsource/
			<br> The Tenth International Conference on Learning Representations, April 2022
			<br />
			<li> <i> MLPerf Tiny Benchmark </i> <br> Colby Banbury, Vijay Janapa Reddi, Peter Torelli, Jeremy Holleman, Nat Jeffries, Csaba Kiraly, Pietro Montino, David Kanter, Sebastian Ahmed, Danilo Pau, <b> Urmish Thakker </b>, Antonio Torrini, Peter Warden, Jay Cordaro, Giuseppe Di Guglielmo, Javier Duarte, Stephen Gibellini, Videet Parekh, Honson Tran, Nhan Tran, Niu Wenxu, Xu Xuesong (<b> NeurIPS 2021 </b>) <br>[<a href="https://arxiv.org/abs/2106.07597">arxiv link</a>][<a href="https://openreview.net/forum?id=8RxxwAut1BI">Openreview Link</a>]
			<br> Thirty-fifth Conference on Neural Information Processing Systems, Dec 2021
			<br />
			<li><i> MicroNets: Neural Network Architectures for Deploying TinyML Applications on Commodity Microcontrollers </i> (<b> MLSys 2021 </b>)<br> Colby Banbury, Chuteng Zhou, Igor Fedorov, Ramon Matas Navarro, <b>Urmish Thakkar</b>, Dibakar Gope, Vijay Janapa Reddi, Matthew Mattina, Paul N. Whatmough <br> 
			[<a href="https://arxiv.org/abs/2010.11267">Arxiv</a>][<a href="https://proceedings.mlsys.org/paper/2021/hash/a3c65c2974270fd093ee8a9bf8ae7d0b-Abstract.html">MLSys Link</a>] <br> Fourth Conference on Machine Learning and Systems, April 2021
			<br />
			<li><i> Doping: A Technique for Extreme Compression of LSTM Models using Sparse Additive Matrices</i>(<b> MLSys 2021 </b>)<br> <b> Urmish Thakker </b>, Paul N. Whatmough, Zhi-Gang Liu, Matthew Mattina, Jesse Beu <br> [<a href="https://proceedings.mlsys.org/paper/2021/hash/a3f390d88e4c41f2747bfa2f1b5f87db-Abstract.html"> MLSys Link </a>][<a href="https://arxiv.org/abs/2102.07071"> Arxiv Link</a>] <br> Fourth Conference on Machine Learning and Systems, April 2021
			<br />
			<li> <i> Compressing RNNs to Kilobyte Budget for IoT Devices Using Kronecker Products </i> (<b> JETC 2021 </b>) <br> <b>Urmish Thakker</b>, Jesse Beu, Dibakar Gope, Chu Zhou, Igor Fedorov, Ganesh Dasika and Matthew Mattina <br> To appear, ACM Journal on Emerging Technologies in Computing Systems, 2021
			<br> [<a href="https://arxiv.org/abs/1906.02876">Arxiv Link</a>][<a href="https://dl.acm.org/doi/abs/10.1145/3440016">ACM Link</a>] 
			<br />
			<li> <i> Federated Learning for Resource-Constrained IoT Devices: Panoramas and State-of-the-art </i> <br> Ahmed Imteaj, <b>Urmish Thakker</b>, Shiqiang Wang, Jian Li and M. Hadi Amini <br> (<b> IOTJ 2021 </b>)
			[<a href="https://arxiv.org/abs/2002.10610">Arxiv</a>] [<a href="https://ieeexplore.ieee.org/document/9475501"> IEEE Link </a>]<br> IEEE Internet of Things Journal, July 2021
			<br />
			<li> <i> Rank and Run-time aware compression of NLP Applications </i> (<b> SustaiNLP-EMNLP 2020 </b>) <br> <b>Urmish Thakker</b>, Jesse Beu, Dibakar Gope, Ganesh Dasika and Matthew Mattina <br> First Workshop on Simple and Efficient Natural Language Processing at The Conference on Empirical Methods in Natural Language Processing (EMNLP), Nov 2020 <br> Links [<a href="https://sites.google.com/view/sustainlp2020/home">Workshop</a>][<a href="https://arxiv.org/abs/2010.03193"> Arxiv Paper </a>][<a href="https://aclanthology.org/2020.sustainlp-1.2/">ACL Link</a>] 
<br />
			<li> <i> Pushing the Envelope of Dynamic Spatial Gating technologies </i> (<b> AIChallengeIoT 2020 </b>) <br> Xueqin Huang, <b> Urmish Thakker </b>, Dibakar Gope, Jesse Beu <br> 2nd International Workshop on Challenges in Artificial Intelligence and Machine Learning for Internet of Things at ACM SenSys, Nov 2020 <br> Links [<a href="https://aichallengeiot.github.io/">Workshop</a>][<a href="https://dl.acm.org/doi/10.1145/3417313.3429380">ACM Link </a>]
			<br />
	<li> <i> Understanding the Impact of Dynamic Channel Pruning on Conditionally Parameterized Convolutions </i> (<b> AIChallengeIoT 2020 </b>) <br> Ravi Raju, Dibakar Gope, <b> Urmish Thakker </b>, Jesse Beu <br> 2nd International Workshop on Challenges in Artificial Intelligence and Machine Learning for Internet of Things at ACM SenSys, Nov 2020 <br> Links [<a href="https://aichallengeiot.github.io/">Workshop</a>][<a href="https://dl.acm.org/doi/10.1145/3417313.3429381"> ACM Link </a>]
			<br />
			<li> <i> Ternary MobileNets via Per-Layer Hybrid Filter Banks </i> (<b> Joint Workshop on Efficient Deep Learning in Computer Vision </b> ) <br> Dibakar Gope, Jesse Beu, <b> Urmish Thakker </b>, Matthew Mattina <br> Joint Workshop on Efficient Deep Learning in Computer Vision at Conference on Computer Vision and Pattern Recognition (CVPR), June 2020 <br> Links [<a href="https://workshop-edlcv.github.io/">Workshop</a>][<a href="https://arxiv.org/abs/1911.01028">Paper</a>]
			<br />
			<li> <i>Pushing the limits of RNN Compression</i> (<b>NeurIPS-EMC2 2019</b>)<br><b>Urmish Thakker</b>, Igor Fedorov, Jesse Beu, Dibakar Gope, Chu Zhou, Ganesh Dasika and Matthew Mattina <br> 
5th Workshop on Energy Efficient Machine Learning and Cognitive Computing, Co-located with the 33rd Conference on Neural Information Processing Systems (NeurIPS), Dec. 2019. <br>
			Links [<a href="https://www.emc2-ai.org/neurips-19">Workshop</a>][<a href="https://arxiv.org/abs/1910.02558">Arxiv Paper</a>][<a href="https://ieeexplore.ieee.org/document/9463545"> IEEE Link</a>]
			<br />
			<li> <i>Skipping RNN State Updates without Retraining the Original Model*</i> (<b>SenSys-ML 2019</b>)<br>
			Jin Tao, <b>Urmish Thakker</b>, Ganesh Dasika, Jesse Beu <br> 
			1st Workshop on Machine Learning on Edge in Sensor Systems (Sensys-ML), Co-located with 17th ACM Conference on Embedded Networked Sensor Systems (SenSys 2019), Nov. 2019<br>
			Links [<a href="https://sensysml.github.io/index">Workshop</a>][<a href="https://dl.acm.org/citation.cfm?id=3362965">Paper</a>]<br>
			*Won the best paper award
			<br />
			<li> <i>Run-Time Efficient RNN Compression for Inference on Edge Device</i> (<b>ISCA-EMC2 2019</b>)<br>
			<b>Urmish Thakker</b>, Jesse Beu, Dibakar Gope, Ganesh Dasika and Matthew Mattina <br>
			4th Workshop on Energy Efficient Machine Learning and Cognitive Computing for Embedded Applications (EMC2), Co-located with the 46th Int. Symp on Computer Architecture (ISCA), Jun. 2019. <br>
			Links [<a href="https://www.emc2-ai.org/isca-19">Workshop</a>][<a href="https://arxiv.org/abs/1906.04886">Paper</a>]
			<br />
			<br />
	<h3>Peer Reviewed Workshop Papers</h3>
			<li> <i> Doping: A Technique for Extreme Compression of LSTM Models using Sparse Additive Matrices </i> (<b> SNN Workshop 2021 </b>) <br> <b>Urmish Thakker</b>, Paul Whatmough, Zhi-Gang Liu, Matthew Mattina, Jesse Beu <br> Sparsity in Neural Networks: Advancing Understanding and Practice, July 2021 <br> Links [<a href="https://sites.google.com/view/sparsity-workshop-2021/">Workshop</a>] <br>
			<li> <i> Doped Structured Matrices for Extreme Compression of LSTM Models </i> (<b> SustaiNLP-EMNLP 2020 </b>) <br> <b>Urmish Thakker</b>, Paul Whatmough, Zhi-Gang Liu, Matthew Mattina, Jesse Beu <br> First Workshop on Simple and Efficient Natural Language Processing at The Conference on Empirical Methods in Natural Language Processing (EMNLP), Nov 2020 <br> Links [<a href="https://sites.google.com/view/sustainlp2020/home">Workshop</a>] <br>
			<li> <i> Benchmarking TinyML Systems: Challenges and Direction* </i> (<b>Benchmarking Machine Learning Workloads on Emerging Hardware Workshop</b>)<br> Colby Banbury , Vijay Janapa Reddi , Will Fu , Max Lam , Amin Fazel , Jeremy Holleman , Xinyuan Huang , Robert Hurtado , David Kanter , Anton Lokhmotov , David Patterson , Danilo Pau , Jeff Sieracki , Jae-Sun Seo , <b> Urmish Thakkar</b>, Marian Verhelst , Poonam Yadav  <br> First International Workshop on Benchmarking Machine Learning Workloads on Emerging Hardware at Third Conference on Machine Learning and Systems (MLSys), March 2020 <br>*As part of the TinyML Performance Working Group<br>Links [<a href="https://memani1.github.io/challenge20/">Workshop</a>][<a href="https://arxiv.org/abs/2003.04821v1">Paper</a>]
			<br />
			<li> <i> Compressing Language Models using Doped Kronecker Products</i> (<b>On-device Intelligence Workshop</b>)<br> <b> Urmish Thakker </b>, Paul Whatmough, Matthew Mattina, Jesse Beu <br> On-device Intelligence Workshop at Third Conference on Machine Learning and Systems (MLSys), March 2020 <br> Links [<a href="https://research.fb.com/programs/on-device-intelligence-workshop/#Accepted_Submissions">Workshop</a>][<a href="https://arxiv.org/abs/2001.08896">Paper</a>][<a href="https://www.youtube.com/watch?v=ZjrQMaMCNLc">Video</a>]
			<br />
			<li> <i>A Static Analysis-based Cross-Architecture Performance Prediction Using Machine Learning </i> (<b>ISCA-AIDArc 2019</b>)<br>
			Newsha Ardalani, <b>Urmish Thakker</b>, Aws Albarghouthi, Karu Sankaralingam <br>
			2nd International Workshop on AI-assisted Design for Architecture co-located with 46th Int. Symposium on Computer Architecture (ISCA), Jun. 2019<br>
			Links [<a href="https://eecs.oregonstate.edu/aidarc/">Workshop</a>][<a href="https://arxiv.org/abs/1906.07840">Paper</a>]
			<br />			
			<li> <i>Measuring scheduling efficiency of RNNs for NLP applications</i> (<b>ISPASS-Fasthpath 2019</b>)<br>
			<b>Urmish Thakker</b>, Ganesh Dasika, Jesse Beu, Matthew Mattina <br>
			6th edition of International Workshop on Performance Analysis of Machine Learning Systems (Fastpath) co-located with IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), March 2019. <br>
			Links [<a href="https://researcher.watson.ibm.com/researcher/view_group.php?id=9888">Workshop</a>][<a href="https://arxiv.org/abs/1904.03302">Paper</a>]
			<br />
			<br />
	
	<h3>Extended Abstracts/ Posters</h3>
			<li> <i> Hardware Aware Dynamic Inference Technologies </i> (<b>tinyML 2021</b>)<br>
				tinyML Summit 2021 
				[<a href="https://www.tinyml.org/event/summit-2021/">Link to Summit</a>][<a href="https://www.youtube.com/watch?v=2NyCIypmuzI&ab_channel=tinyML">Recorded Talk</a>]
			<br />
			<li> <i> Improving accuracy of neural networks compressed using fixed structures via doping </i> (<b>tinyML 2020</b>)<br> <b> Urmish Thakker </b>, Ganesh Dasika, Paul Whatmough, Matthew Mattina, Jesse Beu <br> 
			tinyML Summit 2020
			[<a href="https://www.tinymlsummit.org/">Link to Summit</a>][<a href="https://github.com/Urmish/urmish.github.io/blob/master/_publications/DKP_TinyML_Poster.pdf">Poster</a>]
			<br />
			<li> <i> Aggressive Compression of MobileNets Using Hybrid Ternary Layers </i> (<b>tinyML 2020</b>) <br> Dibakar Gope, Jesse Beu, <b> Urmish Thakker </b>, and Matthew Mattina <br> 
			tinyML Summit 2020
				[<a href="https://www.tinymlsummit.org/">Link to Summit</a>][<a href="https://github.com/Dibakar/dibakar.github.io/blob/master/Posters/tinyML%20Summit%202020%20Poster.pdf">Poster</a>]
			<br />
			<li> <i>RNN Compression using Hybrid Matrix Decomposition</i> (<b>tinyML 2019</b>)<br> 
			<b>Urmish Thakker</b>, Ganesh Dasika, Jesse Beu, Dibakar Gope, and Matthew Mattina <br>
			tinyML Summit, Mar. 2019. <br>
			Links [<a href="https://tinymlsummit.org/2019/">Link to Summit</a>][<a href="https://github.com/Urmish/urmish.github.io/blob/master/_publications/HMD_TinyML.pdf">Poster</a>]
			<br /> <br />

</ol>
